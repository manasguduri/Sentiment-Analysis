{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import KFold\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import pandas as pd\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "import pickle\n",
    "from sklearn import neighbors\n",
    "from zipfile import ZipFile\n",
    "from sklearn import svm\n",
    "from pylab import *\n",
    "import requests\n",
    "import configparser\n",
    "import tweepy\n",
    "from TwitterAPI import TwitterAPI\n",
    "import sys\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "#from urllib.request import urlopen\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets(filename):\n",
    "    tweet_t = []\n",
    "    user_n = []\n",
    "    with open(filename,) as csvfile:\n",
    "        filereader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in filereader:\n",
    "            try:\n",
    "                tweet_t.append(row[2])\n",
    "                user_n.append(row[3])\n",
    "            except IndexError:\n",
    "                pass\n",
    "    return tweet_t,user_n\n",
    "\n",
    "tweet_test, user_name = read_tweets('C:\\\\Users\\\\manas\\\\Desktop\\\\twitter_data.csv')\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.findall(r\"\\w+|\\S\", text.lower(),flags = re.L)\n",
    "    tokens1 = []\n",
    "    for i in tokens:\n",
    "        x = re.findall(r\"\\w+|\\S\", i,flags = re.U)\n",
    "        for j in x:\n",
    "            tokens1.append(j)\n",
    "            \n",
    "    return tokens1\n",
    "\n",
    "tokens = [tokenize(t) for t in tweet_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 24, 152)\n"
     ]
    }
   ],
   "source": [
    "afinn_file = open('AFINN-111.txt', 'r')\n",
    "\n",
    "afinn = dict()\n",
    "\n",
    "for line in afinn_file:\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) == 2:\n",
    "        afinn[parts[0].decode(\"utf-8\")] = int(parts[1])\n",
    "\n",
    "def afinn_sentiment2(terms, afinn, verbose=False):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for t in terms:\n",
    "        if t in afinn:\n",
    "            if verbose:\n",
    "                print('\\t%s=%d' % (t, afinn[t]))\n",
    "            if afinn[t] > 0:\n",
    "                pos += afinn[t]\n",
    "            else:\n",
    "                neg += -1 * afinn[t]\n",
    "    return pos, neg\n",
    "\n",
    "#manually labelling the data\n",
    "positives = []\n",
    "negatives = []\n",
    "all2 = [] \n",
    "all1 = []\n",
    "neutral = []\n",
    "tweet_manual_labelling = []\n",
    "\n",
    "for token_list, tweet in zip(tokens, tweet_test):\n",
    "    pos, neg = afinn_sentiment2(token_list, afinn)\n",
    "    all2.append((tweet, pos, neg))\n",
    "    if pos > neg:\n",
    "        positives.append((tweet, pos, neg))\n",
    "        #print(positives)\n",
    "    elif neg > pos:\n",
    "        negatives.append((tweet, pos, neg))\n",
    "    else:\n",
    "        all1.append((tweet, pos, neg))\n",
    "        \n",
    "for tweet, pos, neg in sorted(positives, key=lambda x: x[1], reverse=True):\n",
    "    neutral.append(('2', tweet))\n",
    "\n",
    "\n",
    "for tweet, pos, neg in sorted(negatives, key=lambda x: x[1], reverse=True):\n",
    "    neutral.append(('1', tweet))\n",
    "\n",
    "for tweet, pos, neg in sorted(all1, key=lambda x: x[1], reverse=True):\n",
    "    neutral.append(('0', tweet))\n",
    "                    \n",
    "with open('neutral.csv', 'w') as fp1:\n",
    "    filewriter = csv.writer(fp1)\n",
    "    filewriter.writerows(neutral)\n",
    "    \n",
    "print(len(positives),len(negatives),len(all1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"tweet_manual_labelling = []\\nfor i in tweet_test:\\n    #if i.split()[0] != 'rt':\\n    tweet_manual_labelling.append((1,i))\\nwith open('tweet_manual_labelling.csv', 'w') as fp1:\\n    filewriter = csv.writer(fp1)\\n    filewriter.writerows(tweet_manual_labelling)\\n\\ntweet_test, user_name = read_tweets('twitter_data.csv')\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''tweet_manual_labelling = []\n",
    "for i in tweet_test:\n",
    "    #if i.split()[0] != 'rt':\n",
    "    tweet_manual_labelling.append((1,i))\n",
    "with open('tweet_manual_labelling.csv', 'w') as fp1:\n",
    "    filewriter = csv.writer(fp1)\n",
    "    filewriter.writerows(tweet_manual_labelling)\n",
    "\n",
    "tweet_test, user_name = read_tweets('twitter_data.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tarining_data(filename):\n",
    "    labeled_tweets = []\n",
    "    labels1 = []\n",
    "    with open(filename, 'rb') as csvfile1:\n",
    "        filereader = csv.reader(csvfile1)\n",
    "        for  row in filereader:\n",
    "            labeled_tweets.append(row[1])\n",
    "            labels1.append(int(row[0]))\n",
    "    return labeled_tweets,np.array(labels1)\n",
    "\n",
    "labeled_tweets,labels = read_tarining_data('neutral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_vectorize(tokenizer_fn=tokenize, min_df=1,\n",
    "                 max_df=1., binary=True, ngram_range=(1,1)):\n",
    "\n",
    "    lis = ['weed','marijuana','pot','cannabis']#list of stop words\n",
    "    #countvectorizer object\n",
    "    vectorizer = CountVectorizer(input = 'content', tokenizer = tokenizer_fn, min_df=min_df, \n",
    "                                     max_df=max_df, binary=binary, ngram_range=ngram_range,\n",
    "                                 dtype = 'int',analyzer='word',token_pattern='(?u)\\b\\w\\w+\\b',encoding='utf-8' )\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix represents 300 documents with 923 features\n",
      "first doc has terms:\n",
      "[27, 93, 151, 209, 226, 501, 570, 620, 773, 797, 798, 811, 845, 862, 895]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = do_vectorize() #get the vectorizer object\n",
    "matrix = vectorizer.fit_transform(labeled_tweets) # learn new words and tranform it to a CSR matrix\n",
    "#print(matrix.toarray())\n",
    "print ('matrix represents %d documents with %d features' % (matrix.shape[0], matrix.shape[1]))\n",
    "print('first doc has terms:\\n%s' % (str(sorted(matrix[0].nonzero()[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fourth shuffled document make memories with maui motorhomes by grabbing this incredible deal this blackfriday for bookings click this_is_a_url has label 0 and terms: [112, 118, 131, 165, 224, 317, 362, 413, 486, 490, 498, 511, 797, 798, 898]\n"
     ]
    }
   ],
   "source": [
    "def repeatable_random(seed):\n",
    "    hash = str(seed)\n",
    "    while True:\n",
    "        hash = hashlib.md5(hash).digest()\n",
    "        #print(type(hash))\n",
    "        for c in hash:\n",
    "            #print(type(c))\n",
    "            yield ord(c)\n",
    "\n",
    "def repeatable_shuffle(X, y, labeled_tweets):\n",
    "    r = repeatable_random(42) \n",
    "    indices = sorted(range(X.shape[0]), key=lambda x: next(r))\n",
    "    #print(indices)\n",
    "    return X[indices], y[indices], np.array(labeled_tweets)[indices]\n",
    "\n",
    "X, y, twittertweets = repeatable_shuffle(matrix, labels, labeled_tweets)\n",
    "\n",
    "print('fourth shuffled document %s has label %d and terms: %s' % \n",
    "      (twittertweets[4], y[4], sorted(X[4].nonzero()[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on training data=1.000\n"
     ]
    }
   ],
   "source": [
    "def accuracy(truth, predicted):\n",
    "    return len(np.where(truth==predicted)[0]) / len(truth)\n",
    "\n",
    "predicted = model.predict(X)\n",
    "print('accuracy on training data=%.3f' % accuracy(y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf(C = 1.,penalty = 'l2', sample_weight=None):\n",
    "    return LogisticRegression(C = C,penalty=penalty,random_state=42)\n",
    "clf_logistic = get_clf()\n",
    "\n",
    "def get_clf_SVC(C = 1.,penalty = 'l2', sample_weight=None):\n",
    "    return LinearSVC(C = C,penalty=penalty,random_state=42)\n",
    "clf_lSVC = get_clf_SVC()\n",
    "\n",
    "\n",
    "\n",
    "def do_cross_validation(X, y,clf, n_folds):\n",
    "    cv = KFold(len(y), n_folds)\n",
    "    accuracies = []\n",
    "    for train_ind, test_ind in cv: \n",
    "        #print(y[train_ind])\n",
    "        #print('\\n')\n",
    "        #y[train_ind].to_csv(path='ytard.csv')\n",
    "        #Z = np.c_[X.reshape(len(X),-1),y.reshape(len(y),-1)]\n",
    "        #new_X = Z[:,:X.size//len(X)].reshape(X.shape)\n",
    "        #new_y=Z[:,X.size//len(X):].reshape(y.shape)\n",
    "        clf.fit(X[train_ind], y[train_ind])\n",
    "        predictions = clf.predict(X[test_ind])\n",
    "        #print(predictions)\n",
    "        accuracies.append(accuracy_score(y[test_ind], predictions))\n",
    "    avg = np.mean(accuracies)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cross validation accuracy for Logistic Regression=81.3 percentage\n",
      "Average cross validation accuracy for LinearSVC=82.3 percentage\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_accuracy = (do_cross_validation(X, y,clf_logistic, 5))*100\n",
    "print('Average cross validation accuracy for Logistic Regression=%.1f percentage' % (logistic_regression_accuracy))\n",
    "\n",
    "linear_regression_accuracy=(do_cross_validation(X, y,clf_lSVC, 5)*100)\n",
    "print('Average cross validation accuracy for LinearSVC=%.1f percentage' % (linear_regression_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\tLogistic_Regression\tLinearSVC\n",
      "0.1\t78.0\t\t\t82.3\n",
      "1.0\t81.3\t\t\t82.3\n",
      "10.0\t82.0\t\t\t82.3\n",
      "50.0\t82.3\t\t\t82.3\n",
      "100.0\t82.0\t\t\t82.3\n"
     ]
    }
   ],
   "source": [
    "C_list = [0.1,1.0,10.,50.,100.]\n",
    "lr_acr = []\n",
    "ls_acr = []\n",
    "for i in C_list:\n",
    "    clf_logistic1 = get_clf(C=i)\n",
    "    clf_lSVC1 = get_clf_SVC(C=i)\n",
    "    lr_acr.append(do_cross_validation(X, y,clf_logistic1,5)*100)\n",
    "    ls_acr.append(do_cross_validation(X, y,clf_lSVC,5)*100)\n",
    "print \"C\\tLogistic_Regression\\tLinearSVC\"\n",
    "for i in range(len(C_list)):\n",
    "    print \"%.1f\\t%.1f\\t\\t\\t%.1f\" %(C_list[i],lr_acr[i],ls_acr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(CLF,trained_CSR,trained_label,untrained_tweets_CSR):\n",
    "    CLF.fit(trained_CSR,trained_label)\n",
    "    predicted = CLF.predict(untrained_tweets_CSR)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tweets(filename):\n",
    "    tweet_t = []\n",
    "    user_n = []\n",
    "    with open(filename, 'rb') as csvfile:\n",
    "        filereader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in filereader:\n",
    "            try:\n",
    "                tweet_t.append(row[2])\n",
    "                user_n.append(row[3])\n",
    "            except IndexError:\n",
    "                pass\n",
    "    return tweet_t,user_n\n",
    "\n",
    "tweet_test1, user_name = read_tweets('twitter_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(x for x in twittertweets)\n",
    "test_tweet_vector = vectorizer.transform(t for t in tweet_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitic_pred_dict = dict(Counter(Tweet_predicted_logistic))\n",
    "linearsvc_pred_dict = dict(Counter(Tweet_predicted_linearSVC))\n",
    "def print_results(dictionary):\n",
    "    for i in dictionary:\n",
    "        if i == 1:\n",
    "            print \"\\tTweets aganist marijuana\\t\\t%d\" %dictionary[i]\n",
    "        elif i == 0:\n",
    "            print \"\\tPro marijuana tweets\\t\\t\\t%d\" %dictionary[i]\n",
    "        elif i == 2:\n",
    "            print \"\\tTweets supporting its Medical use\\t%d\" %dictionary[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "\tPro marijuana tweets\t\t\t73\n",
      "\tTweets aganist marijuana\t\t7\n",
      "\tTweets supporting its Medical use\t20\n"
     ]
    }
   ],
   "source": [
    "print \"Logistic Regression Results\"\n",
    "print_results(logitic_pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Results\n",
      "\tPro marijuana tweets\t\t\t68\n",
      "\tTweets aganist marijuana\t\t7\n",
      "\tTweets supporting its Medical use\t25\n"
     ]
    }
   ],
   "source": [
    "print \"SVC Results\"\n",
    "print_results(linearsvc_pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
